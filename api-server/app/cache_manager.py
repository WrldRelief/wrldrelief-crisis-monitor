"""
ğŸ“¦ Simple File-based Disaster Cache Manager
ê°€ë²¼ìš´ JSON íŒŒì¼ ê¸°ë°˜ ìºì‹± ì‹œìŠ¤í…œ (DB ì—†ì´)
"""

import json
import os
from datetime import datetime, timedelta
from typing import List
import asyncio
import logging
from dataclasses import asdict

from ai_search import DisasterInfo

logger = logging.getLogger(__name__)

class SimpleDisasterCache:
    """íŒŒì¼ ê¸°ë°˜ ì¬í•´ ë°ì´í„° ìºì‹œ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        self.cache_dir = "data"
        self.cache_file = f"{self.cache_dir}/disasters_cache.json"
        self.meta_file = f"{self.cache_dir}/cache_meta.json"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # ë©”ëª¨ë¦¬ ìºì‹œ
        self.memory_cache = []
        self.last_update = None
        
    async def initialize(self):
        """ì‹œì‘ì‹œ íŒŒì¼ì—ì„œ ë¡œë“œ"""
        await self._load_from_file()
        
    async def get_disasters(self, days=7) -> List[DisasterInfo]:
        """ìºì‹œëœ ì¬í•´ ë°ì´í„° ë°˜í™˜ (7ì¼ í•„í„°ë§)"""
        if not self.memory_cache:
            await self._load_from_file()
            
        # 7ì¼ í•„í„°ë§
        cutoff_time = datetime.now() - timedelta(days=days)
        cutoff_timestamp = int(cutoff_time.timestamp())
        
        filtered = [
            d for d in self.memory_cache 
            if d.timestamp >= cutoff_timestamp
        ]
        
        return sorted(filtered, key=lambda x: x.timestamp, reverse=True)
    
    async def update_cache(self, new_disasters: List[DisasterInfo]) -> int:
        """ìºì‹œ ì—…ë°ì´íŠ¸ (ì¤‘ë³µ ì œê±° + ìë™ ì •ë¦¬)"""
        
        # ê¸°ì¡´ ë°ì´í„° ID ì„¸íŠ¸
        existing_ids = {d.id for d in self.memory_cache}
        
        # ìƒˆë¡œìš´ ë°ì´í„°ë§Œ ì¶”ê°€
        added_count = 0
        for disaster in new_disasters:
            if disaster.id not in existing_ids:
                self.memory_cache.append(disaster)
                added_count += 1
        
        # 7ì¼ ì´ìƒ ëœ ë°ì´í„° ìë™ ì •ë¦¬
        cutoff_time = datetime.now() - timedelta(days=7)
        cutoff_timestamp = int(cutoff_time.timestamp())
        
        before_count = len(self.memory_cache)
        self.memory_cache = [
            d for d in self.memory_cache 
            if d.timestamp >= cutoff_timestamp
        ]
        cleaned_count = before_count - len(self.memory_cache)
        
        # íŒŒì¼ì— ì €ì¥
        await self._save_to_file()
        
        self.last_update = datetime.now()
        
        logger.info(f"ğŸ“¦ Cache updated: +{added_count} new, -{cleaned_count} old, total: {len(self.memory_cache)}")
        
        return added_count
    
    async def _load_from_file(self):
        """íŒŒì¼ì—ì„œ ìºì‹œ ë¡œë“œ"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                # DisasterInfo ê°ì²´ë¡œ ë³€í™˜
                disasters_data = data.get("disasters", [])
                self.memory_cache = []
                
                for item in disasters_data:
                    try:
                        # ì¢Œí‘œ ë°ì´í„° ì²˜ë¦¬
                        if 'coordinates' in item and item['coordinates']:
                            coords = item['coordinates']
                            if isinstance(coords, dict):
                                item['coordinates'] = coords
                            else:
                                item['coordinates'] = {"lat": 0.0, "lng": 0.0}
                        else:
                            item['coordinates'] = {"lat": 0.0, "lng": 0.0}
                        
                        disaster = DisasterInfo(**item)
                        self.memory_cache.append(disaster)
                    except Exception as e:
                        logger.warning(f"Failed to load disaster item: {e}")
                        continue
                
                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                if os.path.exists(self.meta_file):
                    with open(self.meta_file, 'r') as f:
                        meta = json.load(f)
                        last_update_str = meta.get("last_update")
                        if last_update_str:
                            self.last_update = datetime.fromisoformat(last_update_str)
                
                logger.info(f"ğŸ“‚ Loaded {len(self.memory_cache)} disasters from cache file")
            else:
                logger.info("ğŸ“‚ No cache file found, starting fresh")
                self.memory_cache = []
                
        except Exception as e:
            logger.error(f"âŒ Failed to load cache: {e}")
            self.memory_cache = []
    
    async def _save_to_file(self):
        """ìºì‹œë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            # ì¬í•´ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            disasters_dict = []
            for disaster in self.memory_cache:
                disaster_dict = asdict(disaster)
                # ì¢Œí‘œ ë°ì´í„° ì •ë¦¬
                if not disaster_dict.get('coordinates'):
                    disaster_dict['coordinates'] = {"lat": 0.0, "lng": 0.0}
                disasters_dict.append(disaster_dict)
            
            # ìºì‹œ ë°ì´í„° ì €ì¥
            cache_data = {
                "disasters": disasters_dict,
                "total_count": len(self.memory_cache),
                "saved_at": datetime.now().isoformat(),
                "version": "1.0"
            }
            
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            meta_data = {
                "last_update": self.last_update.isoformat() if self.last_update else datetime.now().isoformat(),
                "total_disasters": len(self.memory_cache),
                "cache_file_size": os.path.getsize(self.cache_file) if os.path.exists(self.cache_file) else 0
            }
            
            with open(self.meta_file, 'w') as f:
                json.dump(meta_data, f, indent=2)
                
            logger.debug(f"ğŸ’¾ Saved {len(self.memory_cache)} disasters to cache file")
            
        except Exception as e:
            logger.error(f"âŒ Failed to save cache: {e}")
    
    def should_update(self, interval_minutes=10) -> bool:
        """ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•œì§€ í™•ì¸"""
        if not self.last_update:
            return True
            
        time_diff = datetime.now() - self.last_update
        return time_diff.total_seconds() > (interval_minutes * 60)
    
    def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        return {
            "total_disasters": len(self.memory_cache),
            "last_update": self.last_update.isoformat() if self.last_update else None,
            "cache_file_exists": os.path.exists(self.cache_file),
            "cache_file_size": os.path.getsize(self.cache_file) if os.path.exists(self.cache_file) else 0,
            "oldest_disaster": min([d.timestamp for d in self.memory_cache]) if self.memory_cache else None,
            "newest_disaster": max([d.timestamp for d in self.memory_cache]) if self.memory_cache else None
        }
    
    async def force_refresh(self) -> int:
        """ê°•ì œ ìºì‹œ ìƒˆë¡œê³ ì¹¨"""
        from hybrid_search import HybridDisasterEngine
        
        logger.info("ğŸ”„ Force refreshing cache...")
        
        # ìƒˆë¡œìš´ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë°ì´í„° ìˆ˜ì§‘
        engine = HybridDisasterEngine()
        fresh_disasters = await engine.get_initial_disasters(days=7)
        
        # ê¸°ì¡´ ìºì‹œ í´ë¦¬ì–´
        self.memory_cache = []
        
        # ìƒˆ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
        added_count = await self.update_cache(fresh_disasters)
        
        logger.info(f"âœ… Force refresh complete: {added_count} disasters loaded")
        return added_count
    
    async def cleanup_old_data(self, days=7):
        """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
        cutoff_time = datetime.now() - timedelta(days=days)
        cutoff_timestamp = int(cutoff_time.timestamp())
        
        before_count = len(self.memory_cache)
        self.memory_cache = [
            d for d in self.memory_cache 
            if d.timestamp >= cutoff_timestamp
        ]
        cleaned_count = before_count - len(self.memory_cache)
        
        if cleaned_count > 0:
            await self._save_to_file()
            logger.info(f"ğŸ§¹ Cleaned up {cleaned_count} old disasters")
        
        return cleaned_count
